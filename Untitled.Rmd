---
title: "Untitled"
author: "Dingqing Qian"
date: "2019/12/3"
output: html_document
---

```{r Setup, include=F}
knitr::opts_chunk$set(warning = F,error = F,message = F)
require(tidyverse)
require(tidytext)
library(dplyr)
require(caret) # for machine learning
require(recipes) # For preprocessing data
require(rattle) # For nice tree plots
library(lubridate)# to filter and split data 
```

```{r}
library(readr)
library(dplyr)
reviews <- readr::read_csv("yelp.csv")

# remove any rows that contain an empty value
reviews <- na.omit(reviews)

#check duplicate
reviews %>% distinct(text, keep_all = TRUE)

#Create new variable that labels the star rating as recommend (4-5 stars) , or, not recommend (1-3stars)
reviews$recommend_or_not <- ifelse(reviews$stars >= 3, c("recommend"), c("not_recommend"))
reviews$textLength <- nchar(as.character(reviews$text))
```

# Word Tokenization
```{r}
library(tidytext)
words <- reviews %>%
  select(review_id, business_id, stars, text, recommend_or_not, textLength, cool, useful, funny) %>%
  unnest_tokens(word, text) 
  
words <- words %>% 
  anti_join(stop_words,by="word") %>% #remove stopwords
  filter(!str_detect(word,"\\d")) %>% #remove digits
  filter(!str_detect(word,"[:punct:]")) %>% #remove punctuation
  filter(!word %in% stop_words$word, str_detect(word, "^[a-z']+$"))
words
```

# Word Frequency
```{r}
freq <- words %>% 
  group_by(recommend_or_not) %>% 
  count(word,sort=T) %>% 
  ungroup()
```

```{r,fig.align="center",fig.width=10,fig.height=10}
freq %>% 
  group_by(recommend_or_not) %>% 
  top_n(10,n) %>% 
  ggplot(aes(word,n,fill=recommend_or_not)) +
  geom_col(show.legend = F) + 
  coord_flip() +
  facet_wrap(~recommend_or_not,scales = "free", ncol=1) +
  theme(text = element_text(size=30))
```

# Word Cloud
```{r}
library(wordcloud)
recommend <- words %>% 
  filter(recommend_or_not=="recommend")
not_recommend <- words %>% 
  filter(recommend_or_not=="not_recommend")

 recommend %>%
  anti_join(stop_words, by="word") %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 60))

 not_recommend %>%
  anti_join(stop_words, by="word") %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 60))
```


# Sentiment
```{r}
library(lexicon)
library(textdata)
sentiment <- words %>%
  inner_join(get_sentiments("afinn"), by = "word") %>%
  group_by(review_id, stars, recommend_or_not, textLength, cool, useful, funny) %>%
  summarize(sentiment = mean(value))
sentiment
```

# Sentiment scores correlated with positivity ratings
```{r}
library(ggplot2)
theme_set(theme_bw())
ggplot(sentiment, aes(recommend_or_not, sentiment, group = recommend_or_not)) +
  geom_boxplot() +
  ylab("Average sentiment score")


theme_set(theme_bw())
ggplot(sentiment, aes(stars, sentiment, group = stars)) +
  geom_boxplot() +
  ylab("Average sentiment score")
```

# More sentiment analysis
```{r}
counts <- words %>%
  count(review_id, business_id, stars, word) %>%
  ungroup()

summary <- counts %>%
  group_by(word) %>%
  summarize(business = n_distinct(business_id),
            count = n(),
            uses = sum(n),
            average_stars = mean(stars),
            stars = first(stars)) %>%
  ungroup()
```

```{r}
summary2 <- summary %>%
  filter(count >= 200, business >= 10)     #filter words that appear in at least 200 (out of 200000) reviews, filter for ones that appear in at least 10 businesses 
summary2 %>%
  arrange(desc(average_stars)) # most positive words
summary2 %>%
  arrange(average_stars)  # most negative words
```


```{r}
ggplot(summary2, aes(count, average_stars)) +
  geom_point() +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1, hjust = 1) +
  scale_x_log10() +
  geom_hline(yintercept = mean(reviews$stars), color = "red", lty = 2) +
  xlab("Number of reviews") +
  ylab("Average Stars")

ggplot(summary, aes(average_stars, count)) + geom_point()
```


# Machine Learning
```{r}
require(tidyverse)
require(caret) # for machine learning
require(rattle) # For nice tree plots

# For parallelization (to run the models across many cores) -- speeds up computation!
install.packages("doMC")
doMC::registerDoMC()
```

#set tain and test model
```{r}
index = createDataPartition(sentiment$recommend_or_not,p=.8,list=F) 
train_data = sentiment[index,] # Use 80% of the data as training data 
test_data = sentiment[-index,] # holdout 20% as test data 

dim(train_data)
dim(test_data)
```


# pre-process the data
```{r}
rcp <- 
  recipe(recommend_or_not ~ textLength + cool + useful + funny + sentiment, train_data) %>%
  step_knnimpute(all_predictors()) %>% # missing values are imputed
  step_dummy(all_nominal(),-all_outcomes()) %>% # Why exclude outcomes?
  step_range(all_numeric()) %>%  # Normalize scale
  prep()

train_data2 <- bake(rcp,train_data)
test_data2 <- bake(rcp,test_data) 
```

#cross-validation
```{r}
set.seed(1988) # set a seed for replication purposes 

folds <- createFolds(train_data2$recommend_or_not, k = 5) # Partition the data into 5 folds

sapply(folds,length)

control_conditions <- 
  trainControl(method='cv', # K-fold cross validation
               summaryFunction = twoClassSummary, # Need this because it's a classification problem
               classProbs = TRUE, # Need this because it's a classification problem
               index = folds # The indices for our folds (so they are always the same)
  )
```




# Models
## Logistic Regression
```{r}
mod_logit <-
  train(recommend_or_not ~ textLength + cool + useful + funny + sentiment, 
        data=train_data2, # Training data 
        method = "glm", # logit function
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )
mod_logit
```

## K-Nearest Neighbors
```{r}
mod_knn <-
  train(recommend_or_not ~ textLength + cool + useful + funny + sentiment, # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "knn", # K-Nearest Neighbors Algorithm
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )
mod_knn

plot(mod_knn)
```

# CART
```{r}
mod_cart <-
  train(recommend_or_not ~ textLength + cool + useful + funny + sentiment, # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "rpart", # Classification Tree
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )

plot(mod_cart)
```
```{r}
# This tree goes really deep
fancyRpartPlot(mod_cart$finalModel)
print(mod_cart$finalModel)
```


# Random Forest
```{r}
mod_rf <-
  train(recommend_or_not ~ textLength + cool + useful + funny + sentiment, # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "ranger", # random forest (ranger is much faster than rf)
        metric = "ROC", # area under the curve
        trControl = control_conditions
  )
mod_rf
```



```{r}
# Organize all model imputs as a list.
mod_list <-
  list(
    knn = mod_knn,
    logitReg = mod_logit,
    cart = mod_cart,
    rf = mod_rf )

# Generate Plot to compare output. 
dotplot(resamples(mod_list))
```
# Predictive Performance 
```{r}
pred <- predict(mod_logit, newdata = test_data2)
confusionMatrix(table(pred,test_data2$recommend_or_not))
```

